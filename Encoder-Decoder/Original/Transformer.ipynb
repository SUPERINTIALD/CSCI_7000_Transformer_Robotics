{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03135b27-effb-4b88-b268-dc174c8f8b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device not found. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device found. Using GPU.\")\n",
    "else:\n",
    "    print(\"MPS device not found. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "310298c6-ca09-459c-ad47-61ceea2b6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Download names file\n",
    "# !wget https://raw.githubusercontent.com/hackerb9/ssa-baby-names/refs/heads/main/allnames.txt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Load names\n",
    "with open(\"allnames.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "    names = [name.lower() for name in names]\n",
    "\n",
    "# Define alphabet and mappings\n",
    "alphabet = [' '] + sorted(list(set(''.join(names)))) + ['.']\n",
    "itoc = {i: c for i, c in enumerate(alphabet)}\n",
    "ctoi = {c: i for i, c in enumerate(alphabet)}\n",
    "\n",
    "encode = lambda name : [ctoi[c] for c in name]\n",
    "decode = lambda tokens : ''.join([itoc[i] for i in tokens])\n",
    "\n",
    "# Create training and validation set\n",
    "n=int(0.9*len(names))\n",
    "train_data, val_data = random_split(names, [n, len(names)-n])\n",
    "\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        self.ctoi = ctoi\n",
    "        self.alphabet_size = len(alphabet)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        x = [self.ctoi[c] for c in name]  # Convert characters to indices\n",
    "        y = x[1:] + [self.ctoi[' ']]  # The next character to predict (shifted version of x)\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        return x, y  \n",
    "\n",
    "# Define a function to pad sequences\n",
    "def pad_sequences(batch):\n",
    "    max_len = max([len(x) for x, _ in batch])  # Find the max length in the batch\n",
    "    padded_x = []\n",
    "    padded_y = []\n",
    "\n",
    "    for x, y in batch:\n",
    "        padded_x.append(F.pad(x, (0, max_len - len(x)), \"constant\", ctoi[' ']))  # Pad x\n",
    "        padded_y.append(F.pad(y, (0, max_len - len(x)), \"constant\", ctoi['.']))  # Pad y\n",
    "\n",
    "    # Stack the padded sequences to create the batch\n",
    "    return torch.stack(padded_x), torch.stack(padded_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb736a39-adb6-49f5-a677-1f9d785654ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wilona   \n",
      "tensor([23,  9, 12, 15, 14,  1,  0,  0,  0], device='cuda:0')\n",
      "tensor([ 9, 12, 15, 14,  1,  0, 27, 27, 27], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset = NameDataset(train_data)\n",
    "val_dataset = NameDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_sequences)\n",
    "\n",
    "name = next(iter(train_loader)) # Tuple of (x, target)\n",
    "print(decode(name[0].tolist()[0]))\n",
    "print(name[0][0]) # grab the 0th name\n",
    "print(name[1][0]) # grab the 0th target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468f7437-c968-4b84-83e8-52e6d6782f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "n_embd=len(alphabet)\n",
    "\n",
    "x=torch.tensor(encode('laika')).unsqueeze(0) # to add batch dimension\n",
    "\n",
    "xenc=F.one_hot(x, num_classes=n_embd).float()\n",
    "print(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68da66a-e6a4-41ec-a580-d54858e4391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(xenc @ xenc.transpose(-2,-1)) # (5x28) * (28x5) -> (5x5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ae4071-cdf0-49b6-b088-39c3d3789506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print((xenc @ xenc.transpose(-2,-1)) @ xenc) # (1x5x5) x (1x5x28) -> (1x5x28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89bd408-64ad-4e64-af9f-90cb2eb2a591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4046, 0.1488, 0.1488, 0.1488, 0.1488],\n",
       "         [0.1185, 0.3222, 0.1185, 0.1185, 0.3222],\n",
       "         [0.1488, 0.1488, 0.4046, 0.1488, 0.1488],\n",
       "         [0.1488, 0.1488, 0.1488, 0.4046, 0.1488],\n",
       "         [0.1185, 0.3222, 0.1185, 0.1185, 0.3222]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ xenc.transpose(-2,-1)).softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe1de88-d6da-4c72-82f9-8d27b55e6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1488, 0.0000, 0.1488, 0.4046, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.6444, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1185, 0.0000, 0.1185, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4046, 0.0000, 0.1488, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1488, 0.0000, 0.4046, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.6444, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1185, 0.0000, 0.1185, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "attn = (xenc @ xenc.transpose(-2,-1)).softmax(dim=-1) @ xenc\n",
    "print(attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffadb7ec-8f43-4b82-b7ff-0d4a67d726ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  1,  9, 11,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5264bf05-f3a8-458b-81a6-7da93422b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = xenc.shape\n",
    "dk = C\n",
    "\n",
    "query = nn.Linear(C, dk, bias=False)\n",
    "key = nn.Linear(C, dk, bias=False) \n",
    "value = nn.Linear(C, dk, bias=False) \n",
    "\n",
    "Q = query(xenc) # B x T x dk\n",
    "K = key(xenc) # B x T x dk\n",
    "V = value(xenc) # B x T x dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23c9b60-407b-4a9b-af7c-0fa94834a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ((Q @ K.transpose(-2,-1))/(dk**0.5)).softmax(dim=-1) @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51696d6-31b8-4ad4-8a71-bf13125641a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssss'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(attn.argmax(dim=-1)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b19724-15e0-4405-81d6-69a188358b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cftuf\n",
      "jtmed\n",
      "unbpc\n",
      "zxfrr\n",
      "jzbqz\n",
      "vuykl\n",
      "emlfg\n",
      "nm.uk\n",
      "kmckr\n",
      "btoss\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):  \n",
    "  attn_probs = attn.softmax(dim=-1)  # Apply softmax to get probabilities over the vocabulary\n",
    "  sampled_indices = torch.multinomial(attn_probs.view(-1, attn_probs.size(-1)), 1)\n",
    "  print(decode(sampled_indices.T[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc06d599-6a95-4ff6-b4b7-64d13f831982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = embed_dim // num_heads\n",
    "        \n",
    "        # Linear layers for query, key, and value (in the case of cross-attention, separate inputs are used)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, C = q.shape  # Assuming q, k, v have the same shape (B: batch size, T: sequence length, C: embedding dim)\n",
    "        \n",
    "        # Project Q, K, V using their respective linear layers\n",
    "        q = self.q_proj(q)  # Shape: (B, T, C)\n",
    "        k = self.k_proj(k)  # Shape: (B, T, C)\n",
    "        v = self.v_proj(v)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Reshape into (B, num_heads, T, dk)\n",
    "        q = q.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        k = k.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        v = v.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.dk ** 0.5)  # (B, heads, T, T)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = attn_weights @ v  # (B, heads, T, dk)\n",
    "        \n",
    "        # Combine heads back to (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Final linear projection\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "m = MultiHeadAttention(28,4)\n",
    "attn = m(xenc,xenc,xenc)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7872a8b5-e692-4f83-b6aa-54683b615261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=4*28, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, nhead)\n",
    "        \n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),  # First fully connected layer\n",
    "            nn.ReLU(),                          # Non-linearity\n",
    "            nn.Linear(dim_feedforward, d_model)  # Second fully connected layer\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention block\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "        src = self.norm1(src + attn_output)  # Add & Norm\n",
    "        \n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(src)\n",
    "        src = self.norm2(src + self.dropout(ff_output))  # Add & Norm\n",
    "\n",
    "        return src\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(28, 4)\n",
    "output = encoder_layer(xenc)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bffe3e6-bee7-4ea6-835c-32d9623d32fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=16):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a long enough \"position\" tensor\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))  # (embed_dim / 2)\n",
    "        \n",
    "        # Apply the sine and cosine functions\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        \n",
    "        # Register the positional encoding as a buffer (no gradient updates)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        return x + self.pe[:x.size(1)]  # Add the positional encoding to the input tensor\n",
    "\n",
    "m = PositionalEncoding(28)\n",
    "m.forward(xenc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "713e75ff-4d74-428e-96ca-af73a87b3cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmasked attention:\n",
      " tensor([[[0.4046, 0.1488, 0.1488, 0.1488, 0.1488],\n",
      "         [0.1185, 0.3222, 0.1185, 0.1185, 0.3222],\n",
      "         [0.1488, 0.1488, 0.4046, 0.1488, 0.1488],\n",
      "         [0.1488, 0.1488, 0.1488, 0.4046, 0.1488],\n",
      "         [0.1185, 0.3222, 0.1185, 0.1185, 0.3222]]])\n",
      "Masked attention:\n",
      " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2689, 0.7311, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2119, 0.2119, 0.5761, 0.0000, 0.0000],\n",
      "         [0.1749, 0.1749, 0.1749, 0.4754, 0.0000],\n",
      "         [0.1185, 0.3222, 0.1185, 0.1185, 0.3222]]])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = xenc.shape\n",
    "\n",
    "print(f\"Unmasked attention:\\n {(xenc @ xenc.transpose(-2,-1)).softmax(dim=-1)}\")\n",
    "\n",
    "wei = xenc @ xenc.transpose(-2,-1) \n",
    "wei = wei.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1) \n",
    "    \n",
    "print(f\"Masked attention:\\n {wei}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a62a70-5bd0-4c90-919c-27b88b3d6c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = embed_dim // num_heads\n",
    "        \n",
    "        # Linear layers for query, key, and value (in the case of cross-attention, separate inputs are used)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, C = q.shape  # Assuming q, k, v have the same shape (B: batch size, T: sequence length, C: embedding dim)\n",
    "        \n",
    "        # Project Q, K, V using their respective linear layers\n",
    "        q = self.q_proj(q)  # Shape: (B, T, C)\n",
    "        k = self.k_proj(k)  # Shape: (B, T, C)\n",
    "        v = self.v_proj(v)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Reshape into (B, num_heads, T, dk)\n",
    "        q = q.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        k = k.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        v = v.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        \n",
    "        # Scaled dot-product attention with mask\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.dk ** 0.5)  # (B, heads, T, T)\n",
    "        attn_weights = attn_weights.masked_fill(torch.tril(torch.ones(T,T, device=q.device)) == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = attn_weights @ v  # (B, heads, T, dk)\n",
    "        \n",
    "        # Combine heads back to (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Final linear projection\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "m = MaskedMultiHeadAttention(28,4)\n",
    "attn = m(xenc,xenc,xenc)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630a49f5-5460-47b8-97ef-075a9e0ea737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        # Masked Multi-Head Attention\n",
    "        self.self_attention = MaskedMultiHeadAttention(d_model, nhead)\n",
    "        \n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),  # First fully connected layer\n",
    "            nn.ReLU(),                          # Non-linearity\n",
    "            nn.Linear(dim_feedforward, d_model)  # Second fully connected layer\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention block\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "        src = self.norm1(src + attn_output)  # Add & Norm\n",
    "        \n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(src)\n",
    "        src = self.norm2(src + self.dropout(ff_output))  # Add & Norm\n",
    "\n",
    "        return src\n",
    "\n",
    "encoder_layer = TransformerDecoderLayer(28, 4)\n",
    "output = encoder_layer(xenc)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6853aa90-b303-4907-895a-41f924e798e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irwuopbn\n",
      "Model Parameters: 277084\n"
     ]
    }
   ],
   "source": [
    "class RandomNameGenerator(nn.Module):\n",
    "  def __init__(self, d_model, nhead, nlayers, max_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.nhead = nhead\n",
    "    self.embed = nn.Embedding(len(alphabet), d_model)\n",
    "#    self.pe = PositionalEncoding(d_model)\n",
    "    self.wpe = nn.Embedding(max_length,d_model)\n",
    "    self.decoder = nn.ModuleList([TransformerDecoderLayer(d_model, nhead) for _ in range(nlayers)])\n",
    "\n",
    "    self.linear = nn.Linear(d_model, len(alphabet))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T = x.size()\n",
    "\n",
    "    x = self.embed(x)\n",
    "    \n",
    "    #x = self.pe(x)\n",
    "    pos = torch.arange(0, T, dtype=torch.long, device=x.device).unsqueeze(0) # shape (1, t)\n",
    "    x = x + self.wpe(pos)\n",
    "    \n",
    "    for layer in self.decoder:\n",
    "      x = layer(x)\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def generate(self, x, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits = self(x)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = self.softmax(logits)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      if next_token == ctoi[' ']:\n",
    "        break\n",
    "      x = torch.cat((x, next_token), dim=1)\n",
    "    return x[:,1:] # drop the first seed character\n",
    "\n",
    "torch.manual_seed(42)\n",
    "m = RandomNameGenerator(32, 4,2,16).to(device)\n",
    "\n",
    "print(decode(m.generate(torch.tensor([0]).unsqueeze(0).to(device),8).tolist()[0]))\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2acd956d-76ce-47b3-9a87-e52ae40253f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.114407777786255\n",
      "Epoch 1, Loss: 1.8800314664840698\n",
      "Epoch 2, Loss: 1.8707400560379028\n",
      "Epoch 3, Loss: 1.900682806968689\n",
      "Epoch 4, Loss: 1.8413975238800049\n",
      "Epoch 5, Loss: 1.9123412370681763\n",
      "Epoch 6, Loss: 1.8829976320266724\n",
      "Epoch 7, Loss: 1.8685030937194824\n",
      "Epoch 8, Loss: 1.9657368659973145\n",
      "Epoch 9, Loss: 1.8908004760742188\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=5e-4, weight_decay=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for xenc_batch, y_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = m(xenc_batch)\n",
    "    logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * max_seq_len, vocab_size]\n",
    "    y_batch = y_batch.view(-1)  # Shape: [batch_size * max_seq_len]\n",
    "\n",
    "    # Compute the loss using CrossEntropyLoss\n",
    "    loss = F.cross_entropy(logits, y_batch, ignore_index=ctoi['.'])\n",
    "    \n",
    "    # Backward pass\n",
    "    m.zero_grad(set_to_none=True) # make sure ALL the gradients are set to zero\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a7117b0-ef73-4357-ba89-f1de9ee1a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=pad_sequences)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        X, Y = batch\n",
    "        logits = model(X)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        Y = Y.view(-1)  # Shape: [batch_size * max_seq_len]\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss\n",
    "        loss = F.cross_entropy(logits, Y, ignore_index=ctoi['.'])\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae14523f-cd63-4cac-995f-3d9cf56e2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNameGenerator(\n",
      "  (embed): Embedding(28, 32)\n",
      "  (wpe): Embedding(16, 32)\n",
      "  (decoder): ModuleList(\n",
      "    (0-1): 2 x TransformerDecoderLayer(\n",
      "      (self_attention): MaskedMultiHeadAttention(\n",
      "        (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (feedforward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=32, out_features=28, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6763f9f5-b742-42b1-ac9d-a2e6aad3c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(self, x, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = self(x)\n",
    "        logits = logits[:, -1, :] # Look at the last predicted character\n",
    "        probs = self.softmax(logits)\n",
    "        # Randomly sample the next character based on the probability distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if next_token == ctoi[' ']: # Stop if the model predicts a space\n",
    "            break\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "    return x[:, 1:] # Return the generated name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "718cdacc-f803-4a84-8ddc-be1bef11ada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erado\n",
      "ider\n",
      "ulannah\n",
      "ayufthe\n",
      "aylan\n",
      "uisa\n",
      "aiya\n",
      "aelon\n",
      "akibel\n",
      "ato\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Seed with character 0 (often a space or start token)\n",
    "    seed = torch.tensor([0]).unsqueeze(0).to(device)\n",
    "    generated_indices = m.generate(seed, 15).tolist()[0]\n",
    "    print(decode(generated_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4bd409e-e95d-482a-8c94-45347a05ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azaeleen\n",
      "ake\n",
      "rose\n",
      "idile\n",
      "ylien\n",
      "ael\n",
      "a\n",
      "uith\n",
      "adeen\n",
      "ackoben\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Seed with character 0 (often a space or start token)\n",
    "    seed = torch.tensor([0]).unsqueeze(0).to(device)\n",
    "    generated_indices = m.generate(seed, 15).tolist()[0]\n",
    "    print(decode(generated_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e152f1-c7e2-46f6-9367-77a83d1bbd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
